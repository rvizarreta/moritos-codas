{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29598e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble of simple models...\n",
      "Using 12 features\n",
      "Training model 1/3\n",
      "Model 1 Val AUC: 0.9444\n",
      "Training model 2/3\n",
      "Model 2 Val AUC: 0.9466\n",
      "Training model 3/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    \"\"\"Dataset for jet features\"\"\"\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels) if labels is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "        \n",
    "class SimpleJetMLP(nn.Module):\n",
    "    \"\"\"Simple but robust MLP with strong regularization\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32, 16], dropout=0.3):\n",
    "        super(SimpleJetMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"\n",
    "    Load processed data with unique IDs and add derived features.\n",
    "    Returns:\n",
    "        tuple: (X_train, y_train, train_ids, X_val, y_val, val_ids, X_test, y_test, test_ids)\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    X_train = pd.read_csv('data/train/features/cluster_features.csv')\n",
    "    y_train = np.load('data/train/labels/labels.npy')\n",
    "    train_ids = np.load('data/train/ids/ids.npy')\n",
    "    \n",
    "    # Load validation data\n",
    "    X_val = pd.read_csv('data/val/features/cluster_features.csv')\n",
    "    y_val = np.load('data/val/labels/labels.npy')\n",
    "    val_ids = np.load('data/val/ids/ids.npy')\n",
    "    \n",
    "    # Load test data\n",
    "    X_test = pd.read_csv('data/test/features/cluster_features.csv')\n",
    "    test_ids = np.load('data/test/ids/ids.npy')\n",
    "    \n",
    "    # Add derived features to all datasets\n",
    "    X_train = add_derived_features(X_train)\n",
    "    X_val = add_derived_features(X_val)\n",
    "    X_test = add_derived_features(X_test)\n",
    "    \n",
    "    # Return the data\n",
    "    return X_train, y_train, train_ids, X_val, y_val, val_ids, X_test, test_ids\n",
    "\n",
    "def add_derived_features(df):\n",
    "    \"\"\"\n",
    "    Add all derived features to the dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Small epsilon to avoid division by zero\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # 1. Ratio-Based Features\n",
    "    # For pt_fraction_top2, we need to estimate second highest pt\n",
    "    # Since we have cluster_pt_ratio = max/second_max, we can derive: second_max = max/ratio\n",
    "    df['second_highest_pt'] = df['max_cluster_pt'] / (df['cluster_pt_ratio'] + eps)\n",
    "    df['pt_fraction_top2'] = (df['max_cluster_pt'] + df['second_highest_pt']) / (df['total_pt'] + eps)\n",
    "    \n",
    "    # Assuming uniform distribution for top 3 (approximation)\n",
    "    df['pt_fraction_top3'] = df['pt_fraction_top2'] + df['mean_cluster_pt'] / (df['total_pt'] + eps)\n",
    "    df['pt_fraction_top3'] = df['pt_fraction_top3'].clip(upper=1.0)  # Cap at 1\n",
    "    \n",
    "    # Size fractions\n",
    "    total_size = df['n_clusters'] * df['mean_cluster_size']\n",
    "    df['size_fraction_largest'] = df['max_cluster_size'] / (total_size + eps)\n",
    "    \n",
    "    # Mean to max ratios\n",
    "    df['mean_to_max_pt_ratio'] = df['mean_cluster_pt'] / (df['max_cluster_pt'] + eps)\n",
    "    df['mean_to_max_size_ratio'] = df['mean_cluster_size'] / (df['max_cluster_size'] + eps)\n",
    "    \n",
    "    # 2. Concentration Metrics\n",
    "    # For pt concentration, approximate sum of squares\n",
    "    # Using the fact that Var = E[X²] - E[X]², we get E[X²] = Var + E[X]²\n",
    "    mean_pt_squared = df['std_cluster_pt']**2 + df['mean_cluster_pt']**2\n",
    "    sum_pt_squared = mean_pt_squared * df['n_clusters']\n",
    "    df['pt_concentration'] = df['max_cluster_pt']**2 / (sum_pt_squared + eps)\n",
    "    \n",
    "    # Size concentration\n",
    "    mean_size_squared = df['std_cluster_size']**2 + df['mean_cluster_size']**2\n",
    "    sum_size_squared = mean_size_squared * df['n_clusters']\n",
    "    df['size_concentration'] = df['max_cluster_size']**2 / (sum_size_squared + eps)\n",
    "    \n",
    "    # Coefficient of variation\n",
    "    df['cv_cluster_pt'] = df['std_cluster_pt'] / (df['mean_cluster_pt'] + eps)\n",
    "    df['cv_cluster_size'] = df['std_cluster_size'] / (df['mean_cluster_size'] + eps)\n",
    "    \n",
    "    # 3. Asymmetry Features\n",
    "    df['pt_asymmetry'] = (df['max_cluster_pt'] - df['mean_cluster_pt']) / (df['total_pt'] + eps)\n",
    "    df['size_asymmetry'] = (df['max_cluster_size'] - df['mean_cluster_size']) / (total_size + eps)\n",
    "    df['spatial_asymmetry'] = np.abs(df['mean_cluster_eta'] - df['mean_cluster_phi'])\n",
    "    \n",
    "    # 4. Normalized Features\n",
    "    df['normalized_max_pt'] = df['max_cluster_pt'] / (df['total_pt'] + eps)\n",
    "    df['normalized_std_pt'] = df['std_cluster_pt'] / (df['total_pt'] + eps)\n",
    "    df['clusters_per_gev'] = df['n_clusters'] / (df['total_pt'] + eps)\n",
    "    \n",
    "    # 5. Combined Spatial-Energy Features\n",
    "    df['eta_weighted_pt'] = df['max_cluster_eta'] * df['max_cluster_pt'] / (df['total_pt'] + eps)\n",
    "    df['phi_weighted_pt'] = df['max_cluster_phi'] * df['max_cluster_pt'] / (df['total_pt'] + eps)\n",
    "    df['spatial_extent'] = df['max_cluster_eta'] * df['max_cluster_phi']\n",
    "    \n",
    "    # 6. Statistical Moments\n",
    "    df['pt_skewness'] = (df['max_cluster_pt'] - df['mean_cluster_pt']) / (df['std_cluster_pt'] + eps)\n",
    "    df['size_skewness'] = (df['max_cluster_size'] - df['mean_cluster_size']) / (df['std_cluster_size'] + eps)\n",
    "    \n",
    "    # 7. Logarithmic Features\n",
    "    df['log_cluster_pt_ratio'] = np.log(df['cluster_pt_ratio'] + 1)  # log(x+1) to handle zeros\n",
    "    df['log_n_clusters'] = np.log(df['n_clusters'] + 1)\n",
    "    df['log_total_pt'] = np.log(df['total_pt'] + 1)\n",
    "    \n",
    "    # 8. Binary/Categorical Features\n",
    "    df['is_single_cluster'] = (df['n_clusters'] == 1).astype(int)\n",
    "    df['has_dominant_cluster'] = (df['cluster_pt_ratio'] > 3).astype(int)\n",
    "    \n",
    "    # PT categories (you can adjust thresholds based on your data distribution)\n",
    "    df['pt_category_low'] = (df['total_pt'] < df['total_pt'].quantile(0.33)).astype(int)\n",
    "    df['pt_category_high'] = (df['total_pt'] > df['total_pt'].quantile(0.67)).astype(int)\n",
    "    \n",
    "    # 9. Interaction Features\n",
    "    df['pt_size_correlation'] = (df['normalized_max_pt'] * df['size_fraction_largest'])\n",
    "    df['spatial_spread'] = np.sqrt(df['mean_cluster_eta']**2 + df['mean_cluster_phi']**2)\n",
    "    df['n_clusters_squared'] = df['n_clusters']**2\n",
    "    \n",
    "    # 10. Inverse Features\n",
    "    df['inv_n_clusters'] = 1 / (df['n_clusters'] + eps)\n",
    "    df['inv_cluster_pt_ratio'] = 1 / (df['cluster_pt_ratio'] + eps)\n",
    "    \n",
    "    # 11. Additional useful features\n",
    "    df['pt_variance_ratio'] = df['std_cluster_pt']**2 / (df['mean_cluster_pt']**2 + eps)\n",
    "    df['size_variance_ratio'] = df['std_cluster_size']**2 / (df['mean_cluster_size']**2 + eps)\n",
    "    \n",
    "    # Energy balance indicator\n",
    "    df['energy_balance'] = 1 - df['normalized_max_pt']\n",
    "    \n",
    "    # Relative spreads\n",
    "    df['relative_eta_spread'] = df['max_cluster_eta'] / (df['mean_cluster_eta'] + eps)\n",
    "    df['relative_phi_spread'] = df['max_cluster_phi'] / (df['mean_cluster_phi'] + eps)\n",
    "\n",
    "    # Add these high-impact angular features\n",
    "    df['phi_variance'] = df['max_cluster_phi']**2 - df['mean_cluster_phi']**2\n",
    "    df['eta_phi_correlation'] = df['mean_cluster_eta'] * df['mean_cluster_phi']\n",
    "    df['angular_asymmetry'] = (df['max_cluster_phi'] - df['mean_cluster_phi']) / (df['max_cluster_phi'] + eps)\n",
    "    \n",
    "    # Drop intermediate columns that were only used for calculations\n",
    "    df = df.drop(columns=['second_highest_pt'], errors='ignore')\n",
    "    \n",
    "    # Handle any potential infinities or NaNs\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "class EnsembleJetClassifier:\n",
    "    \"\"\"Ensemble of simple models to reduce overfitting\"\"\"\n",
    "    def __init__(self, n_models=5):\n",
    "        self.n_models = n_models\n",
    "        self.models = []\n",
    "        self.scalers = []\n",
    "        \n",
    "    def train_single_model(self, X_train, y_train, X_val, y_val, seed=42):\n",
    "        \"\"\"Train a single model with different initialization\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Use RobustScaler for better generalization\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = JetDataset(X_train_scaled, y_train)\n",
    "        val_dataset = JetDataset(X_val_scaled, y_val)\n",
    "        \n",
    "        # Smaller batch size for better generalization\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "        \n",
    "        # Create model\n",
    "        model = SimpleJetMLP(X_train.shape[1], hidden_dims=[32, 16], dropout=0.4)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Training setup with L2 regularization\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        best_val_auc = 0\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            # Training\n",
    "            model.train()\n",
    "            for batch_features, batch_labels in train_loader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            val_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_labels in val_loader:\n",
    "                    batch_features = batch_features.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "                    \n",
    "                    outputs = model(batch_features)\n",
    "                    val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    val_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            val_auc = roc_auc_score(val_labels, val_preds)\n",
    "            \n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        return model, scaler, best_val_auc\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train ensemble of models\"\"\"\n",
    "        # Use only the most important features to reduce overfitting\n",
    "        important_features = ['max_cluster_phi', 'spatial_spread', 'n_clusters', \n",
    "                            'phi_weighted_pt', 'total_pt', 'mean_cluster_pt',\n",
    "                            'cluster_pt_ratio', 'max_cluster_eta', 'mean_cluster_phi',\n",
    "                            'spatial_extent', 'normalized_max_pt', 'pt_concentration']\n",
    "        \n",
    "        # Filter to available features\n",
    "        available_features = [f for f in important_features if f in X_train.columns]\n",
    "        X_train_filtered = X_train[available_features]\n",
    "        X_val_filtered = X_val[available_features]\n",
    "        self.features = available_features\n",
    "\n",
    "        print(f\"Using {len(available_features)} features\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "            model, scaler, val_auc = self.train_single_model(\n",
    "                X_train_filtered, y_train, X_val_filtered, y_val, seed=42+i*100\n",
    "            )\n",
    "            self.models.append(model)\n",
    "            self.scalers.append(scaler)\n",
    "            print(f\"Model {i+1} Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Get ensemble validation score\n",
    "        ensemble_preds = self.predict_proba(X_val)\n",
    "        ensemble_auc = roc_auc_score(y_val, ensemble_preds)\n",
    "        print(f\"\\nEnsemble Val AUC: {ensemble_auc:.4f}\")\n",
    "        \n",
    "        self.features = available_features\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get ensemble predictions\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        X_filtered = X[self.features]\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        for model, scaler in zip(self.models, self.scalers):\n",
    "            X_scaled = scaler.transform(X_filtered)\n",
    "            dataset = JetDataset(X_scaled)\n",
    "            loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "            \n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    batch = batch.to(device)\n",
    "                    outputs = model(batch)\n",
    "                    predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "        # Average predictions\n",
    "        return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# Alternative: Use original BDT with conservative settings\n",
    "def train_conservative_xgboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train XGBoost with conservative parameters to avoid overfitting\"\"\"\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    # Use only original features\n",
    "    original_features = ['n_clusters', 'total_pt', 'max_cluster_size', 'mean_cluster_size', \n",
    "                        'std_cluster_size', 'max_cluster_pt', 'mean_cluster_pt', 'std_cluster_pt',\n",
    "                        'max_cluster_eta', 'max_cluster_phi', 'mean_cluster_eta', 'mean_cluster_phi',\n",
    "                        'cluster_pt_ratio', 'cluster_size_ratio']\n",
    "    \n",
    "    X_train_orig = X_train[original_features]\n",
    "    X_val_orig = X_val[original_features]\n",
    "    \n",
    "    # Conservative parameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        min_child_weight=5,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=1,\n",
    "        reg_lambda=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_orig, y_train)\n",
    "    val_pred = model.predict_proba(X_val_orig)[:, 1]\n",
    "    val_auc = roc_auc_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"Conservative XGBoost Val AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return model, original_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X_train, y_train, train_ids, X_val, y_val, val_ids, X_test, test_ids = load_processed_data()\n",
    "    \n",
    "    print(\"Training ensemble of simple models...\")\n",
    "    ensemble = EnsembleJetClassifier(n_models=5)\n",
    "    ensemble.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Generate test predictions\n",
    "    test_predictions = ensemble.predict_proba(X_test)\n",
    "    \n",
    "    # Save predictions\n",
    "    solution = pd.DataFrame({'id': test_ids, 'label': test_predictions})\n",
    "    solution.to_csv('solution_ensemble_simple.csv', index=False)\n",
    "    print(\"Predictions saved to solution_ensemble_simple.csv\")\n",
    "    \n",
    "    # Also try conservative XGBoost\n",
    "    print(\"\\nTraining conservative XGBoost...\")\n",
    "    xgb_model, xgb_features = train_conservative_xgboost(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    test_predictions_xgb = xgb_model.predict_proba(X_test[xgb_features])[:, 1]\n",
    "    solution_xgb = pd.DataFrame({'id': test_ids, 'label': test_predictions_xgb})\n",
    "    solution_xgb.to_csv('solution_xgb_conservative.csv', index=False)\n",
    "    print(\"XGBoost predictions saved to solution_xgb_conservative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c86a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d34658",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JetClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m JetClassifier()  \u001b[38;5;66;03m# or however you create the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JetClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "model = JetClassifier()  # or however you create the model\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8feac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
